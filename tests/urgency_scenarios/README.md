# Tests de Calibration des Urgences Guardian

Ce dossier contient une suite complÃ¨te de tests pour calibrer et valider l'Ã©valuation des niveaux d'urgence par l'IA Guardian (Gemini).

## ğŸ“‚ Structure

```
urgency_scenarios/
â”œâ”€â”€ scenarios_data.py           # Base de donnÃ©es de 40+ scÃ©narios
â”œâ”€â”€ test_urgency_calibration.py # Suite de tests automatisÃ©e
â”œâ”€â”€ interactive_trainer.py      # EntraÃ®neur interactif
â””â”€â”€ README.md                    # Ce fichier
```

## ğŸ¯ Objectifs

1. **Calibration prÃ©cise** : S'assurer que Guardian Ã©value correctement le niveau de danger (1-10)
2. **PrÃ©venir les faux positifs** : Ã‰viter d'alerter les proches pour des incidents mineurs (ex: crevaison)
3. **DÃ©tecter les vraies urgences** : Garantir que les situations critiques dÃ©clenchent les bonnes alertes
4. **Validation continue** : Tester aprÃ¨s chaque modification du prompt ou de la logique

## ğŸ“Š Base de donnÃ©es de scÃ©narios

### `scenarios_data.py`

Contient **40+ scÃ©narios rÃ©els** rÃ©partis en catÃ©gories :

#### ğŸŸ¢ Faible (1-3) - Pas d'urgence
- Crevaison de vÃ©lo
- Question d'information
- ProblÃ¨me technique mineur
- **â†’ Aucun email envoyÃ©**

#### ğŸŸ¡ ModÃ©rÃ©e (4-5) - Attention
- Personne perdue
- Petit malaise
- Petite blessure
- **â†’ Pas d'email (surveillance)**

#### ğŸŸ  Ã‰levÃ©e (6-7) - Intervention nÃ©cessaire
- Chute avec douleur
- Coupure importante
- Menace de sÃ©curitÃ©
- **â†’ Email envoyÃ© aux proches**

#### ğŸ”´ Critique (8-10) - Danger immÃ©diat
- DÃ©tresse respiratoire
- Agression
- Accident grave
- Perte de conscience
- **â†’ Email + SMS + Alertes complÃ¨tes**

#### ğŸ¤” Cas ambigus
- Situations Ã  interprÃ©ter selon le contexte
- Permet de tester la finesse de l'IA

## ğŸ§ª Utilisation des outils

### 1. Tests automatisÃ©s complets

Lance tous les tests et gÃ©nÃ¨re un rapport :

```bash
cd tests/urgency_scenarios
python3 test_urgency_calibration.py
```

**Options :**
- `--category faible` : Tester une catÃ©gorie spÃ©cifique
- `--max-tests 10` : Limiter le nombre de tests
- `--delay 2.0` : DÃ©lai entre tests (Ã©viter rate limiting)
- `--export` : Exporter les rÃ©sultats en JSON

**Exemples :**

```bash
# Tester uniquement les scÃ©narios faibles
python3 test_urgency_calibration.py --category faible

# Tester 5 scÃ©narios avec export
python3 test_urgency_calibration.py --max-tests 5 --export

# Tester les critiques avec dÃ©lai de 2s
python3 test_urgency_calibration.py --category critique --delay 2.0
```

### 2. EntraÃ®neur interactif

Mode interactif pour tester rapidement :

```bash
python3 interactive_trainer.py
```

**Commandes :**
- Entrez une situation : analyse immÃ©diate
- `examples` : Voir des exemples
- `stats` : Statistiques de session
- `quit` : Quitter

**Tests rapides prÃ©dÃ©finis :**

```bash
python3 interactive_trainer.py --quick
```

**Test d'une situation spÃ©cifique :**

```bash
python3 interactive_trainer.py --test "Je suis tombÃ© Ã  vÃ©lo et j'ai crevÃ©"
```

## ğŸ“ˆ InterprÃ©tation des rÃ©sultats

### Rapport de test

```
ğŸ“Š RAPPORT FINAL - CALIBRATION DES URGENCES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ˆ Statistiques globales:
  Total testÃ©:        40
  âœ… Parfait (exact): 28 (70.0%)
  âœ“ OK (Â±1):          10 (25.0%)
  âŒ Incorrect:       2 (5.0%)
  âš ï¸  Erreurs:         0

ğŸ¯ Taux de rÃ©ussite global: 95.0%
   ğŸŒŸ EXCELLENT - Calibration trÃ¨s prÃ©cise!
```

### CritÃ¨res de qualitÃ©

- **>90%** : ğŸŒŸ Excellent
- **75-90%** : âœ… Bon
- **60-75%** : âš ï¸ Moyen (ajustements nÃ©cessaires)
- **<60%** : âŒ Faible (revoir la logique)

## ğŸ”§ Ajustement de la calibration

Si les tests rÃ©vÃ¨lent des problÃ¨mes :

### 1. Modifier les mots-clÃ©s (simulation mode)

Fichier : `guardian/gemini_agent.py`

```python
# Ajouter des mots-clÃ©s pour situations NON urgentes
non_urgent_indicators = [
    'crevaison', 'crevÃ©', 'panne', 'pneu',
    'vÃ©lo cassÃ©', 'mÃ©canique', 'Ã§a va', 'pas grave'
]
```

### 2. Ajuster le prompt Gemini (API mode)

Fichier : `guardian/gemini_agent.py`, fonction `analyze_emergency_situation()`

```python
prompt = f"""...
IMPORTANT - Ã‰chelle de gravitÃ©:
- Niveau 1-3 (Faible): ProblÃ¨mes mineurs...
- Niveau 4-6 (ModÃ©rÃ©e): Situations inconfortables...
...
"""
```

### 3. Ajouter de nouveaux scÃ©narios

Fichier : `tests/urgency_scenarios/scenarios_data.py`

```python
"faible": [
    {
        "description": "Nouvelle situation Ã  tester",
        "niveau_attendu": 2,
        "categorie": "Faible",
        "email_attendu": False,
        ...
    },
]
```

## ğŸ“Š Statistiques actuelles

La base de donnÃ©es contient :
- **Total** : 40+ scÃ©narios
- **Faible** : 8 scÃ©narios (20%)
- **ModÃ©rÃ©e** : 7 scÃ©narios (17.5%)
- **Ã‰levÃ©e** : 7 scÃ©narios (17.5%)
- **Critique** : 8 scÃ©narios (20%)
- **Ambigus** : 5 scÃ©narios (12.5%)
- **Psychologique** : 3 scÃ©narios (7.5%)

## ğŸ“ Bonnes pratiques

1. **Tester aprÃ¨s chaque modification** du code de calibration
2. **Ajouter des scÃ©narios** basÃ©s sur les cas rÃ©els rencontrÃ©s
3. **Maintenir la balance** entre les catÃ©gories
4. **Exporter les rÃ©sultats** pour suivre l'Ã©volution
5. **Documenter les cas problÃ©matiques** pour amÃ©lioration

## ğŸš€ IntÃ©gration CI/CD

Pour automatiser les tests :

```bash
# Dans votre pipeline CI/CD
python3 test_urgency_calibration.py --max-tests 20 --export
# VÃ©rifier que le taux de rÃ©ussite > 80%
```

## ğŸ“ Contribuer

Pour ajouter des scÃ©narios :

1. Ã‰diter `scenarios_data.py`
2. Ajouter le scÃ©nario dans la bonne catÃ©gorie
3. DÃ©finir le niveau attendu (1-10)
4. Indiquer si email doit Ãªtre envoyÃ©
5. Tester avec `python3 test_urgency_calibration.py --category <votre_categorie>`

## ğŸ› Signaler un problÃ¨me

Si un scÃ©nario est mal Ã©valuÃ© :

1. Noter la situation exacte
2. Niveau attendu vs obtenu
3. Lancer le mode interactif pour analyser
4. Ajuster les mots-clÃ©s ou le prompt
5. Re-tester

---

**Auteur** : Guardian AI Team  
**DerniÃ¨re mise Ã  jour** : 10 novembre 2025  
**Version** : 1.0.0
